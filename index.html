<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Brave new ideas in motion representations</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/main.css">
    <link rel="stylesheet" href="css/sl-slide.css">

    <script src="js/vendor/modernizr-2.6.2-respond-1.1.0.min.js"></script>
    <script src="data/id2path.json" charset="utf-8"></script>
    

    <script src="js/animate.js"></script>

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="images/ico/favicon.ico">
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="images/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="images/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="images/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="images/ico/apple-touch-icon-57-precomposed.png">
</head>

<body>

    <!--Header-->
    <header class="navbar navbar-fixed-top">
        <div class="navbar-inner">
            <div class="container">
                <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </a>
                <a id="logo" class="pull-left" href="index.html"></a>
                <div class="nav-collapse collapse pull-right">
                    <ul class="nav">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="#about">About</a></li>
                        <li><a href="#program">Program</a></li>
                        <li><a href="#dates">Dates</a></li>
                        <li><a href="#submission">Submission</a></li> 
                        <li><a href="#registration">Registration & Venue</a></li>
                        <li><a href="#organizers">Organizers</a></li>
                    </ul>        
                </div><!--/.nav-collapse -->
            </div>
        </div>
    </header>
    <!-- /header -->

    <!--Slider-->
    <section id="slide-show">
     <div id="slider" class="sl-slider-wrapper">

        <!--Slider Items-->    
        <div class="sl-slider">
            <!--Slider Item1-->
            <div class="sl-slide item1" data-orientation="horizontal" data-slice1-rotation="-25" data-slice2-rotation="-25" data-slice1-scale="2" data-slice2-scale="2">
                <div class="sl-slide-inner">
                    <div class="container">
                        <canvas class="pull-right" height="420px" width="800px" id="aniCanvas"></canvas>
                        <h2>Wanted:</h2>
                        <h3 class="gap">New representations of motion</h3>
                    </div>
                </div>
            </div>
            <!--/Slider Item1-->

    </div>
    <!--/Slider Items-->


</div>
<!-- /slider-wrapper -->           
</section>
<!--/Slider-->

<section id="about" >
    <div class="container">
        <div class="row-fluid">
            <div class="span9">
                <h1>CVPR'17 Workshop: <br/>Brave new ideas for motion representations in videos II</h1>
                <p class="lead">Together with the <a href="http://cvpr2017.thecvf.com/">Computer Vision and Pattern Recognition (CVPR)</a> 2017.
                
                This is the second time we organize this workshop following  the succesful <a href="http://bravenewmotion.github.io/eccv16/index.html">previous workshop</a> at ECCV 2016.</p> 
                

                <h3>Description of the workshop and its relevance</h3>
                <p class="lead">
                In the late years Deep Learning has been a great force of change on most computer vision tasks. In video analysis problems, however, such as action recognition and detection, motion analysis and tracking, shallow architectures remain surprisingly competitive. What is the reason for this conundrum?  Larger datasets are part of the solution. The recently proposed Sports1M helped recently in the realistic training of large motion networks. Still, the breakthrough has not yet arrived.
                </p>

                <p class="lead">
                Assuming that the recently proposed video datasets are large enough for training deep networks for video, another likely culprit for the standstill in video analysis is the capacity of the existing deep models. More specifically, the existing deep networks for video analysis might not be sophisticated enough to address the complexity of motion information. This makes sense, as videos introduce an exponential complexity as compared to static images. Unfortunately, state-of-the-art motion representation models are extensions of existing image representations rather than motion dedicated ones. Brave, new and motion-specific representations are likely to be needed for a breakthrough in video analysis.
                </p>


                <h3>Calling papers for brave new ideas</h3>
                <p class="lead">
                Attempting to publish a wild, but intriguing idea can be daunting, resulting in slow progress. On one hand a controversial new idea might be rejected by top-tier conferences, without the right experimental justification. On the other hand, researchers may not want to reveal a smart idea too soon in the fear of not receiving the right credit. To make amends with these two factors, the workshop will admit maximum 8-page papers describing novel, previously unseen ideas without necessarily requiring exhaustive quantitative justifications. Authors can also submit 4 page brief papers but those will not be included in the proceedings (please refer to the instructions given bellow). Moreover, to make sure proper accreditation is given in the future, the workshop will have an open-review process, where all submitted papers should first be uploaded to arXiv.    
                </p>

                <h3>Expert speakers</h3>
                <p class="lead">
                To kickstart the discussion we have confirmed speakers from different fields, details will follow.
                </p>

                <h3>Topics</h3>
                <p class="lead">
                The workshop focuses on motion representations related, but not limited, to the following topics: </p>
               
               <p class="lead">
                - Influence of motion in object recognition, object affordance, scene understanding.<br/>
                - Object and optical flow<br/>
                - Motion prediction, causal reasoning and forecasting<br/>
                - Event and action recognition <br/>
                - Spatio-temporal action localization<br/>
                - Modeling human motion in videos and video streams<br/>
                - Motion segmentation and saliency<br/>
                - Tracking of objects in space and time<br/>
                - Unsupervised action, actom discovery using ego motion<br/>
                - Applications of motion understanding and video dynamics in sports, healthcare, autonomous driving, driver assistance and robotics<br/>
                </p>

                
            </div>
        </div>
    </div>
</section>



<section id="services">   

<div class="container" id ="program">
        <div class="row-fluid">
            <div class="span9">
                <h1>Program</h1>
                <p class="lead">
                Date: 21 july 2017.
                </p>
		<table class="lead" style="border-style: groove;width:1200px" border="1" >
		<tr>
			<th width="20%">Time</th>
    			<th width="40%">Event</th>
    			<th width="40%">Description</th>
  		</tr>
		<tr><td>8.45 - 9.00   </td> <td> Welcome to the workshop </td> <td> Information</td> </tr>
                <tr><td>9.00 - 10.00   </td> <td> Invited speaker 1  </br> <a href="http://people.csail.mit.edu/mrub/"> Dr. Miki Rubinstein</a> </td> <td> Talk 1 </td> </tr>
		<tr><td>10.00 - 10.20  </td> <td> Break </td> <td> Coffee </td> </tr>
                <tr><td>10.20 - 11.20  </td> <td> Invited speaker 2 </br>   <a href="http://cis.jhu.edu/~rvidal/">Professor Rene Vidal</a> </td> <td>  Talk 2 </td> </tr>
                <tr><td>11.25 - 11.45  </td> <td> <a href="https://openreview.net/pdf?id=HJfDVHkag">Unsupervised Human Action Detection by Action Matching</a> </td> <td>Oral Presentation 1 by Basura Fernando, Sareh Shirazi, Stephen Gould </td> </tr>
		<tr><td>11.45 - 13.30  </td> <td> Lunch (on your own) </td> <td> Poster session </td> </tr>

                <tr><td>13.30 - 14.15  </td> <td> Invited speaker 4 </br>  <a href="http://www.cs.rochester.edu/u/jluo/">Professor Jiebo Luo</a>  </td> <td> Talk 4  </td> </tr>
                <tr><td>14.15 - 14.30  </td> <td> <a href="https://openreview.net/pdf?id=BJrN0GhTl">RATM: Recurrent Attentive Tracking Model</a> </td> <td> Oral Presentation 2 by Samira Ebrahimi Kahou, Vincent Michalski, Roland Memisevic, Christopher Pal, Pascal Vincent </td> </tr>
		<tr><td>14.30 - 14.45  </td> <td> Break </td> <td> Coffee </td> </tr>
                <tr><td>14.45 - 15.30  </td> <td> Invited speaker 5 </br> <a href="http://www.ceessnoek.info/">Professor Cees G.M. Snoek</a></td> <td> Talk 5 </td> </tr>

                <tr><td>15.30 - 15.45  </td> <td> <a href="https://openreview.net/pdf?id=BJWBzOBTx">Interpretable 3D Human Action Analysis with Temporal Convolutional Networks</a> </td> <td> Oral Presentation 3 by Tae Soo Kim, Austin Reiter </td> </tr>
                <tr><td>15.45 - 16.00  </td> <td> <a href="https://openreview.net/pdf?id=H1gaV4mae">Learning Dynamic GMM for Attention Distribution on Single-face Videos </a> </td> <td> Oral Presentation 4 by Yun Ren, Zulin Wang, Mai Xu, Haoyu Dong, Shengxi Li </td> </tr>
                <!--tr><td>16.00 - 16.15  </td> <td> <a href="https://openreview.net/pdf?id=SJcvXCBax">Optical Acceleration for Motion Description in Videos </a></td> <td> Oral Presentation 5 by Anitha Edison, Jiji C. V. </td> </tr-->
                <tr><td>16.00 - 16.45  </td> <td> Invited speaker 3 </br>  <a href="http://www.stat.ucla.edu/~yuille/">Professor Alan Yuille </a>  </td> <td>  Talk 3 </td> </tr>
		<tr><td>16.45 - 17.30  </td> <td> Poster session  </td> <td> Poster session  </td> </tr>		
		</table>
                
            </div>
        </div>
    </div>

</section>



<section id="dates">
    <div class="container" >
        <div class="center">
            <h3>Important Dates</h3>
            <p class="lead">Together with the <a href="http://cvpr2017.thecvf.com/">Computer Vision and Pattern Recognition (CVPR)</a> 2017.</p>
            <p class="lead">Date of the workshop: 21 July 2017 </p>
        </div>  
        <div class="gap"></div>
        <ul class="gallery col-4">
            <!--Item 1-->	     
	
	    <!--Item 1.1-->
	    <li>	
             <p class="lead">4 Page Submission Deadline</p>
                <div class="desc">
                    <h5 style="color:red;"><strike>15th May 2017 </strike></h5>
                </div>
            </li>
            <!--/Item 1.1--> 	 

            <!--Item 2-->
            <li>
                <p class="lead">4 Page Acceptance</p>
                <div class="desc">
                    <h5><strike>22nd May 2017</strike></h5>
                </div>
            </li>

            <li>
                <p class="lead">8 Page Acceptance</p>
                <div class="desc">
                    <h5><strike>3rd May 2017</strike></h5>
                </div>
            </li>	
            <!--/Item 2-->

            <!--Item 3-->
            <li>
                <p class="lead">8 Page Camera ready</p>
                <div class="desc">
                    <h5><strike>15th May 2017</strike></h5>
                </div>
            </li>
            <!--/Item 3--> 

                          

        </ul>
    </div>

</section>

<section id="services">
    <div class="container" id="submission">
        <div class="row-fluid">
            <div class="span9">

                <h1>Submission</h1>                
                <h3>Constructive discussion</h3>
                <p class="lead">
                The workshop's goal is a constructive, creative and open conversation. In principle we will accept all papers. All reviews will be made publicly available. Reviewers can choose to remain anonymous or to reveal their identity to encourage collaboration and positive feedback. We include poster presentations and will select a few of the best and bravest papers for an oral presentation. 
                </p>

		<h3>Instructions</h3>                
                <p class="lead">
		You can submit papers in two different formats. <br><br><strike>
1.Full paper submission should include 8 pages of text and should use the CVPR 2017 camera ready format as per the instructions given <a href="http://cvpr2017.thecvf.com/submission/main_conference/author_guidelines">here</a>. Full  paper submission should include 8 pages (excluding references) and will be included in the proceedings of the CVPR17 workshops. Therefore, the deadline for full paper submission is 7th April 2017.<br><br></strike>

2. Authors can also submit 4 Page papers which will be peer reviewed. However, they will not be include in the proceedings. Please follow the 
 CVPR 2017 camera ready format as per the instructions given <a href="http://cvpr2017.thecvf.com/submission/main_conference/author_guidelines">here</a> but limit your paper to 4 pages excluding references.
<br><br>

All papers should have the names of the authors, institute and the email address in the header of the paper as per the camera ready format of CVPR 2017. Authors are encouraged to upload their papers in archive.
                </p>

                <h3>Submit</h3>                
                <p class="lead">
                Please use <a href="https://openreview.net/group?id=cv-foundation.org/CVPR/2017/BNMW">OpenReview</a> to submit your paper.

                </p>

                <h3>Proceedings</h3>                
                <p class="lead">
                Will appear soon.
                </p>


                
            </div>
        </div>
    </div>
</section>



<section id="registration">
    <div class="container">
        <div class="row-fluid">
            <div class="span9">
                <h1>Registration & venue</h1>
                <p class="lead">The workshop is together with the <a href="http://cvpr2017.thecvf.com/">Computer Vision and Pattern Recognition (CVPR)</a> 2017.</p>
                <p class="lead">
                 Accepted papers must have at least one registered author (this can be a student). 
                 </p>


                <p class="lead">
                Venue TBD.</p>
                
            </div>
        </div>
    </div>
</section>



<section id="services">
    <div class="container" id="organizers">
        <div class="row-fluid">
            <div class="span9">
                <h1>Organizers</h1>
                <p class="lead">
                <a href="http://www.egavves.com/"> Stratis Gavves</a>,
                <a href="http://users.cecs.anu.edu.au/~basura/">Basura Fernando</a>,
                <a href="https://www.cs.rochester.edu/u/cxu22/">  Chenliang Xu</a>,
                <a href="https://sites.google.com/site/tomyyan555/">  Yan Yan</a>,
                <a href="http://www.robots.ox.ac.uk/~hbilen/">  Hakan Bilen</a>,
                <a href="https://xmhe.bitbucket.io/">  Xuming He</a>,
                <a href="https://sites.google.com/site/michaelyingyang/"> Michael Ying Yang</a>,
                <a href="http://jvgemert.github.io/"> Jan van Gemert.</a> 
                </p>

                
            </div>
        </div>
    </div>
</section>



<!--Footer-->
<footer id="footer">
    <div class="container">
        <div class="row-fluid">
            <div class="span5 cp">
                &copy; 2013 <a target="_blank" href="http://shapebootstrap.net/" title="Free Twitter Bootstrap WordPress Themes and HTML templates">ShapeBootstrap</a>. All Rights Reserved.
            </div>
            <!--/Copyright-->

            <div class="span1">
                <a id="gototop" class="gototop pull-right" href="#"><i class="icon-angle-up"></i></a>
            </div>
            <!--/Goto Top-->
        </div>
    </div>
</footer>
<!--/Footer-->

<script src="js/vendor/jquery-1.9.1.min.js"></script>
<script src="js/vendor/bootstrap.min.js"></script>
<script src="js/main.js"></script>
<!-- Required javascript files for Slider -->
<script src="js/jquery.ba-cond.min.js"></script>
<script src="js/jquery.slitslider.js"></script>
<!-- /Required javascript files for Slider -->

<!-- SL Slider -->
<script type="text/javascript"> 
$(function() {
    var Page = (function() {

        var $navArrows = $( '#nav-arrows' ),
        slitslider = $( '#slider' ).slitslider( {
            autoplay : false
        } ),

        init = function() {
            initEvents();
        },
        initEvents = function() {
            $navArrows.children( ':last' ).on( 'click', function() {
                slitslider.next();
                return false;
            });

            $navArrows.children( ':first' ).on( 'click', function() {
                slitslider.previous();
                return false;
            });
        };

        return { init : init };

    })();

    Page.init();
});
</script>
<!-- /SL Slider -->
</body>
</html>
